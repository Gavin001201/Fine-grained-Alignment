model:
  base_learning_rate: 4.5e-6
  target: taming.models.vqgan.VQModel
  params:
    ckpt_path: /mnt/workspace/Project/Fine-grained-Alignment/logs/coco_ckpt/coco_epoch117.ckpt
    embed_dim: 256
    n_embed: 8192
    ddconfig:
      # double_z: False
      # z_channels: 256
      # resolution: 256
      # in_channels: 3
      # out_ch: 3
      # ch: 128
      # ch_mult: [ 1,1,2,2,4]  # num_down = len(ch_mult)-1
      # num_res_blocks: 2
      # attn_resolutions: [16]
      # dropout: 0.0
      # vocab_size: 49408
      embed_dim: 512
    ctconfig:                #cliptransformer_config
      # context_length:  256
      # vocab_size: 49408
      # width: 512
      # heads: 4
      # layers: 10
      # quick_gelu: True
      # pad_id: 0
    lossconfig:
      target: taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator

data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 48
    num_workers: 8
    train:
      target: taming.data.coco_clip.CocoClipTrain
    validation:
      target: taming.data.coco_clip.CocoClipVal
